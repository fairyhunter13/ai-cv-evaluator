services:
  migrate:
    image: ghcr.io/fairyhunter13/ai-cv-evaluator-migrate:latest
    env_file:
      - ./.env.production
    environment:
      - DB_URL=postgres://${DB_USER:-postgres}:${DB_PASSWORD:-postgres}@db:5432/${DB_NAME:-app}?sslmode=disable
    depends_on:
      db:
        condition: service_healthy
    restart: "no"

  backend_blue: &backend_base
    image: ghcr.io/fairyhunter13/ai-cv-evaluator-server:latest
    env_file:
      - ./.env.production
    environment:
      - APP_ENV=prod
      - DB_URL=postgres://${DB_USER:-postgres}:${DB_PASSWORD:-postgres}@db:5432/${DB_NAME:-app}?sslmode=disable
      - KAFKA_BROKERS=redpanda:9092
      - QDRANT_URL=http://qdrant:6333
      - TIKA_URL=http://tika:9998
      - OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317
      - OPENROUTER_MIN_INTERVAL=5s
      - ADMIN_USERNAME=${ADMIN_USERNAME}
      - ADMIN_PASSWORD=${ADMIN_PASSWORD}
      - ADMIN_SESSION_SECRET=${ADMIN_SESSION_SECRET}
      - CORS_ALLOW_ORIGINS=${CORS_ALLOW_ORIGINS:-https://ai-cv-evaluator.web.id,https://dashboard.ai-cv-evaluator.web.id}
    networks:
      default:
        aliases:
          - app  # Alias for Prometheus to scrape using the same name as dev environment
    depends_on:
      migrate:
        condition: service_completed_successfully
      db:
        condition: service_healthy
      redpanda:
        condition: service_healthy
      qdrant:
        condition: service_started
      tika:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '1.0'
        reservations:
          memory: 256M
          cpus: '0.25'

  backend_green:
    <<: *backend_base

  frontend:
    image: ghcr.io/fairyhunter13/ai-cv-evaluator-frontend:latest
    depends_on:
      - backend_blue
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.1'

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./deploy/nginx/prod-conf.d:/etc/nginx/conf.d:ro
      - ./deploy/nginx/prod-nginx.conf:/etc/nginx/nginx.conf:ro
      - ./deploy/portal:/usr/share/nginx/html:ro
      - /etc/letsencrypt:/etc/letsencrypt:ro
      - /var/www/certbot:/var/www/certbot:ro
    depends_on:
      - frontend
      - backend_blue
      - oauth2-proxy-app
      - oauth2-proxy-dashboard
      - authelia
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 64M
          cpus: '0.1'

  worker:
    image: ghcr.io/fairyhunter13/ai-cv-evaluator-worker:latest
    env_file:
      - ./.env.production
    environment:
      - APP_ENV=prod
      - DB_URL=postgres://${DB_USER:-postgres}:${DB_PASSWORD:-postgres}@db:5432/${DB_NAME:-app}?sslmode=disable
      - KAFKA_BROKERS=redpanda:9092
      - QDRANT_URL=http://qdrant:6333
      - OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317
      - CONSUMER_MAX_CONCURRENCY=1
      - OPENROUTER_MIN_INTERVAL=5s
    depends_on:
      migrate:
        condition: service_completed_successfully
      db:
        condition: service_healthy
      redpanda:
        condition: service_healthy
      qdrant:
        condition: service_started
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.25'
      replicas: 1

  db:
    image: postgres:16-alpine
    environment:
      POSTGRES_PASSWORD: ${DB_PASSWORD:-postgres}
      POSTGRES_USER: ${DB_USER:-postgres}
      POSTGRES_DB: ${DB_NAME:-app}
    volumes:
      - db_data:/var/lib/postgresql/data
    restart: unless-stopped
    command: postgres -c shared_buffers=256MB -c work_mem=16MB -c max_connections=100
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres}"]
      interval: 10s
      timeout: 3s
      retries: 10
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'

  redpanda:
    image: docker.redpanda.com/redpandadata/redpanda:v24.3.1
    command:
      - redpanda
      - start
      - --kafka-addr internal://0.0.0.0:9092,external://0.0.0.0:19092
      - --advertise-kafka-addr internal://redpanda:9092,external://localhost:19092
      - --pandaproxy-addr internal://0.0.0.0:8082,external://0.0.0.0:18082
      - --advertise-pandaproxy-addr internal://redpanda:8082,external://localhost:18082
      - --schema-registry-addr internal://0.0.0.0:8081,external://0.0.0.0:18081
      - --rpc-addr redpanda:33145
      - --advertise-rpc-addr redpanda:33145
      - --smp 2
      - --memory 1536M
      - --default-log-level=info
    volumes:
      - redpanda_data:/var/lib/redpanda/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "rpk cluster health | grep -E 'Healthy:.+true' || exit 1"]
      interval: 15s
      timeout: 3s
      retries: 5
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
        reservations:
          memory: 1536M
          cpus: '1.0'

  redpanda-console:
    image: docker.redpanda.com/redpandadata/console:v2.7.2
    entrypoint: /bin/sh
    command: -c 'echo "$$CONSOLE_CONFIG_FILE" > /tmp/config.yml; /app/console'
    environment:
      CONFIG_FILEPATH: /tmp/config.yml
      CONSOLE_CONFIG_FILE: |
        server:
          basePath: /redpanda
        kafka:
          brokers: ["redpanda:9092"]
          schemaRegistry:
            enabled: true
            urls: ["http://redpanda:8081"]
        redpanda:
          adminApi:
            enabled: true
            urls: ["http://redpanda:9644"]
    depends_on:
      - redpanda
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.1'

  qdrant:
    image: qdrant/qdrant:latest
    volumes:
      - qdrant_data:/qdrant/storage
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.25'
    healthcheck:
      test: ["CMD-SHELL", "exit 0"]
      interval: 30s
      timeout: 5s
      retries: 3

  tika:
    image: apache/tika:2.9.0.0
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 256M
          cpus: '0.25'
    healthcheck:
      test: ["CMD-SHELL", "exit 0"]
      interval: 30s
      timeout: 10s
      retries: 3

  otel-collector:
    image: otel/opentelemetry-collector:0.98.0
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./deploy/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    environment:
      - GOMEMLIMIT=400MiB
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
    healthcheck:
      test: ["CMD-SHELL", "exit 0"]
      interval: 30s
      timeout: 5s
      retries: 3
  jaeger:
    image: jaegertracing/all-in-one:1.57
    environment:
      - QUERY_BASE_PATH=/jaeger
      - MEMORY_MAX_TRACES=5000
      - SPAN_STORAGE_TYPE=memory
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    restart: unless-stopped

  mailpit:
    image: axllent/mailpit:latest
    command: ["--smtp", "0.0.0.0:1025", "--listen", "0.0.0.0:8025", "--webroot", "/mailpit/"]
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.25'
        reservations:
          memory: 64M
          cpus: '0.05'

  prometheus:
    image: prom/prometheus:v2.53.0
    volumes:
      - ./deploy/prometheus.prod.yml:/etc/prometheus/prometheus.yml:ro
      - ./deploy/prometheus-rules.yml:/etc/prometheus/prometheus-rules.yml:ro
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=7d"
      - "--storage.tsdb.retention.size=2GB"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.console.templates=/usr/share/prometheus/consoles"
      - "--web.external-url=/prometheus/"
      - "--web.route-prefix=/prometheus"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'

  node-exporter:
    image: prom/node-exporter:v1.8.1
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)'
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 64M
          cpus: '0.2'
        reservations:
          memory: 32M
          cpus: '0.1'

  # Docker Meta Exporter - Helper for Friendly Names
  meta-exporter:
    build: ./deploy/exporter
    container_name: ai-cv-evaluator-meta-exporter
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    ports:
      - "8000:8000"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.2'

  grafana:
    image: grafana/grafana:11.1.0
    volumes:
      - ./deploy/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ./deploy/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./deploy/grafana/provisioning/alerting:/etc/grafana/provisioning/alerting:ro
      - ./deploy/grafana/dashboards:/etc/grafana/dashboards:ro
    environment:
      - GF_SERVER_ROOT_URL=%(protocol)s://%(domain)s/grafana/
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      - GF_AUTH_DISABLE_LOGIN_FORM=true
      - GF_AUTH_PROXY_ENABLED=true
      - GF_AUTH_PROXY_HEADER_NAME=X-Forwarded-User
      - GF_AUTH_PROXY_AUTO_SIGN_UP=true
      - GF_USERS_AUTO_ASSIGN_ORG=true
      - GF_USERS_AUTO_ASSIGN_ORG_ROLE=Admin
      - GF_SMTP_ENABLED=true
      - GF_SMTP_HOST=mailpit:1025
      - GF_SMTP_FROM_ADDRESS=alerts@ai-cv-evaluator.local
      - GF_SMTP_FROM_NAME=AI CV Evaluator Alerts
      - GF_SMTP_SKIP_VERIFY=true
      - GF_SMTP_STARTTLS_POLICY=NoStartTLS
    depends_on:
      - prometheus
      - loki
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'

  # Authelia (Identity Provider)
  authelia:
    image: authelia/authelia:4.38.19
    container_name: ai-cv-evaluator-authelia
    environment:
      TZ: "UTC"
      AUTHELIA_SERVER_SCHEME: http
      AUTHELIA_SERVER_ADDRESS: tcp://0.0.0.0:9091
    volumes:
      - ./deploy/authelia/configuration.prod.yml:/config/configuration.yml:ro
      - ./deploy/authelia/users_database.yml:/config/users_database.yml:ro
      - ./deploy/authelia/oidc.key:/config/oidc.key:ro
    ports:
      - "9091:9091" # Internal/Local access if needed, but primarily proxied via Nginx
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.1'
    healthcheck:
      test: ["CMD-SHELL", "authelia healthcheck"]
      interval: 30s
      timeout: 3s
      retries: 5

  # oauth2-proxy for main app domain
  oauth2-proxy-app:
    image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
    env_file:
      - ./.env.production
    environment:
      OAUTH2_PROXY_PROVIDER: oidc
      # Authelia URL (Production Domain)
      OAUTH2_PROXY_OIDC_ISSUER_URL: https://auth.ai-cv-evaluator.web.id
      OAUTH2_PROXY_LOGIN_URL: https://auth.ai-cv-evaluator.web.id/api/oidc/authorization
      # Internal Docker Network Access
      OAUTH2_PROXY_REDEEM_URL: http://authelia:9091/api/oidc/token
      OAUTH2_PROXY_OIDC_JWKS_URL: http://authelia:9091/jwks.json
      OAUTH2_PROXY_USER_INFO_URL: http://authelia:9091/api/oidc/userinfo
      OAUTH2_PROXY_CLIENT_ID: oauth2-proxy
      OAUTH2_PROXY_CLIENT_SECRET: ${OAUTH2_PROXY_CLIENT_SECRET:?set in .env.production}
      OAUTH2_PROXY_COOKIE_SECRET: ${OAUTH2_PROXY_COOKIE_SECRET:?set in .env.production}
      OAUTH2_PROXY_COOKIE_SECURE: "true"
      OAUTH2_PROXY_COOKIE_SAMESITE: lax
      OAUTH2_PROXY_COOKIE_DOMAINS: "ai-cv-evaluator.web.id,.ai-cv-evaluator.web.id"
      OAUTH2_PROXY_EMAIL_DOMAINS: ${OAUTH2_PROXY_EMAIL_DOMAINS:-*}
      OAUTH2_PROXY_SET_XAUTHREQUEST: "true"
      OAUTH2_PROXY_REDIRECT_URL: https://ai-cv-evaluator.web.id/oauth2/callback
      OAUTH2_PROXY_UPSTREAMS: static://200
      OAUTH2_PROXY_HTTP_ADDRESS: 0.0.0.0:4180
      OAUTH2_PROXY_WHITELIST_DOMAINS: ".ai-cv-evaluator.web.id"
      OAUTH2_PROXY_INSECURE_OIDC_ALLOW_UNVERIFIED_EMAIL: "true"
      OAUTH2_PROXY_INSECURE_OIDC_SKIP_ISSUER_VERIFICATION: "true"
      OAUTH2_PROXY_SKIP_OIDC_DISCOVERY: "true"
      OAUTH2_PROXY_OIDC_GROUPS_CLAIM: "groups"
      OAUTH2_PROXY_SKIP_PROVIDER_BUTTON: "true"
    depends_on:
      - authelia
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.25'
        reservations:
          memory: 64M
          cpus: '0.05'

  # oauth2-proxy for dashboard subdomain
  oauth2-proxy-dashboard:
    image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
    env_file:
      - ./.env.production
    environment:
      OAUTH2_PROXY_PROVIDER: oidc
      # Authelia URL (Production Domain)
      OAUTH2_PROXY_OIDC_ISSUER_URL: https://auth.ai-cv-evaluator.web.id
      OAUTH2_PROXY_LOGIN_URL: https://auth.ai-cv-evaluator.web.id/api/oidc/authorization
      # Internal Docker Network Access
      OAUTH2_PROXY_REDEEM_URL: http://authelia:9091/api/oidc/token
      OAUTH2_PROXY_OIDC_JWKS_URL: http://authelia:9091/jwks.json
      OAUTH2_PROXY_USER_INFO_URL: http://authelia:9091/api/oidc/userinfo
      OAUTH2_PROXY_CLIENT_ID: oauth2-proxy
      OAUTH2_PROXY_CLIENT_SECRET: ${OAUTH2_PROXY_CLIENT_SECRET:?set in .env.production}
      OAUTH2_PROXY_COOKIE_SECRET: ${OAUTH2_PROXY_COOKIE_SECRET:?set in .env.production}
      OAUTH2_PROXY_COOKIE_SECURE: "true"
      OAUTH2_PROXY_COOKIE_SAMESITE: lax
      OAUTH2_PROXY_COOKIE_DOMAINS: "ai-cv-evaluator.web.id,.ai-cv-evaluator.web.id"
      OAUTH2_PROXY_EMAIL_DOMAINS: ${OAUTH2_PROXY_EMAIL_DOMAINS:-*}
      OAUTH2_PROXY_SET_XAUTHREQUEST: "true"
      OAUTH2_PROXY_REDIRECT_URL: https://dashboard.ai-cv-evaluator.web.id/oauth2/callback
      OAUTH2_PROXY_UPSTREAMS: static://200
      OAUTH2_PROXY_HTTP_ADDRESS: 0.0.0.0:4180
      OAUTH2_PROXY_WHITELIST_DOMAINS: ".ai-cv-evaluator.web.id"
      OAUTH2_PROXY_INSECURE_OIDC_ALLOW_UNVERIFIED_EMAIL: "true"
      OAUTH2_PROXY_INSECURE_OIDC_SKIP_ISSUER_VERIFICATION: "true"
      OAUTH2_PROXY_SKIP_OIDC_DISCOVERY: "true"
      OAUTH2_PROXY_OIDC_GROUPS_CLAIM: "groups"
      OAUTH2_PROXY_SKIP_PROVIDER_BUTTON: "true"
    depends_on:
      - authelia
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.25'
        reservations:
          memory: 64M
          cpus: '0.05'

  certbot:
    image: certbot/certbot:latest
    volumes:
      - /etc/letsencrypt:/etc/letsencrypt
      - /var/www/certbot:/var/www/certbot
    restart: unless-stopped
    entrypoint: /bin/sh
    command:
      - -c
      - |
        set -e
        echo "[certbot] Starting automatic TLS renewal loop..."
        # Run renew in a loop so certificates are kept up to date.
        while :; do
          certbot renew --webroot -w /var/www/certbot --keep-until-expiring || true
          echo "[certbot] Renewal check completed; sleeping for 12h..."
          sleep 12h
        done
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.25'
        reservations:
          memory: 64M
          cpus: '0.05'

  # cAdvisor - Container monitoring
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.0
    container_name: ai-cv-evaluator-cadvisor
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
      - /sys/fs/cgroup:/sys/fs/cgroup:ro
    privileged: true
    pid: host
    devices:
      - /dev/kmsg

    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.1'

  # Centralized logging with Loki
  loki:
    image: grafana/loki:3.0.0
    command: ["-config.file=/etc/loki/config.yml"]
    volumes:
      - ./deploy/loki-config.yml:/etc/loki/config.yml:ro
      - loki_data:/loki
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
    healthcheck:
      test: ["CMD-SHELL", "exit 0"]
      interval: 15s
      timeout: 5s
      retries: 3

  # Log shipper - collects container logs and sends to Loki
  promtail:
    image: grafana/promtail:3.0.0
    command: ["-config.file=/etc/promtail/config.yml"]
    volumes:
      - ./deploy/promtail-config.yml:/etc/promtail/config.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/log:/var/log:ro
    depends_on:
      loki:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.05'

volumes:
  db_data:
  redpanda_data:
  qdrant_data:
  loki_data: