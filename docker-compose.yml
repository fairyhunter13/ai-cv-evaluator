services:
  migrate:
    build:
      context: .
      dockerfile: deploy/docker/Dockerfile.migrate
    image: ghcr.io/fairyhunter13/ai-cv-evaluator:dev-migrate
    depends_on:
      db:
        condition: service_healthy
    env_file:
      - ./.env
    environment:
      - DB_URL=postgres://postgres:postgres@db:5432/app?sslmode=disable
    restart: "no"

  app:
    build: 
      context: .
      dockerfile: deploy/docker/Dockerfile.server
    image: ghcr.io/fairyhunter13/ai-cv-evaluator:dev-server
    depends_on:
      migrate:
        condition: service_completed_successfully
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
      qdrant:
        condition: service_started
      tika:
        condition: service_started
      redpanda:
        condition: service_healthy
    env_file:
      - ./.env
    environment:
      - APP_ENV=dev
      - MAX_UPLOAD_MB=1
      # Keep container port fixed at 8080; host port is configured in the ports mapping
      - PORT=8080
      - DB_URL=postgres://postgres:postgres@db:5432/app?sslmode=disable
      - REDIS_ADDR=redis:6379
      - KAFKA_BROKERS=redpanda:9092
      - QDRANT_URL=http://qdrant:6333
      - TIKA_URL=http://tika:9998
      - OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317
      - RATE_LIMIT_PER_MIN=60
      - OPENROUTER_MIN_INTERVAL=1s
      - ADMIN_USERNAME=${ADMIN_USERNAME}
      - ADMIN_PASSWORD=${ADMIN_PASSWORD}
      - ADMIN_SESSION_SECRET=${ADMIN_SESSION_SECRET:-dev-admin-session-secret-change-me}
    ports:
      - "8080:8080"
    restart: unless-stopped

  frontend:
    build:
      context: .
      dockerfile: deploy/docker/dev/Dockerfile.frontend
    image: ghcr.io/fairyhunter13/ai-cv-evaluator:dev-frontend
    environment:
      - NODE_ENV=development
      - VITE_API_BASE_URL=/
    volumes:
      - ./admin-frontend:/app
      - /app/node_modules
    restart: unless-stopped

  worker:
    build: 
      context: .
      dockerfile: deploy/docker/Dockerfile.worker
    image: ghcr.io/fairyhunter13/ai-cv-evaluator:dev-worker
    depends_on:
      migrate:
        condition: service_completed_successfully
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
      redpanda:
        condition: service_healthy
      qdrant:
        condition: service_started
    env_file:
      - ./.env
    environment:
      - APP_ENV=dev
      - DB_URL=postgres://postgres:postgres@db:5432/app?sslmode=disable
      - REDIS_ADDR=redis:6379
      - KAFKA_BROKERS=redpanda:9092
      - QDRANT_URL=http://qdrant:6333
      - OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317
      # Single worker with increased concurrency for optimal throughput
      # Replaces previous multi-worker setup (4 workers × 4 concurrency = 16 total)
      # Now: 1 worker × 24 concurrency = 24 total (50% increase)
      - CONSUMER_MAX_CONCURRENCY=4
      - OPENROUTER_MIN_INTERVAL=1s
      # Per-job evaluation timeout for E2E and local runs. This is consumed by the
      # worker's evaluation handler via the E2E_AI_TIMEOUT environment variable.
      # Default is 4m to stay under the 5-minute SLA unless explicitly overridden.
      - E2E_AI_TIMEOUT=${E2E_AI_TIMEOUT:-4m}
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
        reservations:
          memory: 1G
          cpus: '1.0'
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    volumes:
      - redis_data:/data
    restart: unless-stopped

  db:
    image: postgres:16-alpine
    environment:
      POSTGRES_PASSWORD: postgres
      POSTGRES_USER: postgres
      POSTGRES_DB: app
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 10s
      timeout: 3s
      retries: 10
    volumes:
      - db_data:/var/lib/postgresql/data

  redpanda:
    image: docker.redpanda.com/redpandadata/redpanda:v24.3.1
    command:
      - redpanda
      - start
      - --kafka-addr internal://0.0.0.0:9092,external://0.0.0.0:19092
      - --advertise-kafka-addr internal://redpanda:9092,external://localhost:19092
      - --pandaproxy-addr internal://0.0.0.0:8082,external://0.0.0.0:18082
      - --advertise-pandaproxy-addr internal://redpanda:8082,external://localhost:18082
      - --schema-registry-addr internal://0.0.0.0:8081,external://0.0.0.0:18081
      - --rpc-addr redpanda:33145
      - --advertise-rpc-addr redpanda:33145
      - --mode dev-container
      - --smp 2  # Increased CPU cores for better performance
      - --default-log-level=info
      - --memory 512M  # Optimized memory allocation
      - --reserve-memory 256M  # Reserve memory for system
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    healthcheck:
      test: ["CMD-SHELL", "rpk cluster health | grep -E 'Healthy:.+true' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    volumes:
      - redpanda_data:/var/lib/redpanda/data

  redpanda-console:
    image: docker.redpanda.com/redpandadata/console:v2.7.2
    entrypoint: /bin/sh
    command: -c 'echo "$$CONSOLE_CONFIG_FILE" > /tmp/config.yml; /app/console'
    environment:
      CONFIG_FILEPATH: /tmp/config.yml
      CONSOLE_CONFIG_FILE: |
        server:
          basePath: /redpanda
        kafka:
          brokers: ["redpanda:9092"]
          schemaRegistry:
            enabled: true
            urls: ["http://redpanda:8081"]
        redpanda:
          adminApi:
            enabled: true
            urls: ["http://redpanda:9644"]
    depends_on:
      - redpanda
    volumes:
      - ./deploy/redpanda/console-config.yaml:/tmp/config.yaml:ro

  qdrant:
    image: qdrant/qdrant:latest
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:6333/collections"]
      interval: 10s
      timeout: 3s
      retries: 10
    volumes:
      - qdrant_data:/qdrant/storage

  tika:
    image: apache/tika:2.9.0.0
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:9998/version"]
      interval: 10s
      timeout: 3s
      retries: 10
    # Do not expose in prod; app connects via internal network only
    restart: unless-stopped

  otel-collector:
    image: otel/opentelemetry-collector:0.98.0
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./deploy/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    restart: unless-stopped
    depends_on:
      - jaeger

  jaeger:
    image: jaegertracing/all-in-one:1.57
    environment:
      - QUERY_BASE_PATH=/jaeger
      - MEMORY_MAX_TRACES=5000
      - SPAN_STORAGE_TYPE=memory
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  prometheus:
    image: prom/prometheus:v2.53.0
    volumes:
      - ./deploy/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./deploy/prometheus-rules.yml:/etc/prometheus/prometheus-rules.yml:ro
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.console.templates=/usr/share/prometheus/consoles"
      - "--web.external-url=/prometheus/"
      - "--web.route-prefix=/prometheus"

  # cAdvisor - Container monitoring
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.0
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    command:
      - "--housekeeping_interval=15s"
      - "--store_container_labels=true"
      - "--whitelisted_container_labels=com.docker.compose.service,com.docker.compose.project"
    restart: unless-stopped

  loki:
    image: grafana/loki:3.0.0
    command: ["-config.file=/etc/loki/config.yml"]
    volumes:
      - ./deploy/loki-config.yml:/etc/loki/config.yml:ro
    restart: unless-stopped

  promtail:
    image: grafana/promtail:3.0.0
    command: ["-config.file=/etc/promtail/config.yml"]
    volumes:
      - ./deploy/promtail-config.yml:/etc/promtail/config.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    depends_on:
      - loki
    restart: unless-stopped

  mailpit:
    image: axllent/mailpit:latest
    command: ["--smtp", "0.0.0.0:1025", "--listen", "0.0.0.0:8025", "--webroot", "/mailpit/"]
    # Web UI is proxied behind SSO at /mailpit/ by dev-nginx; local port is
    # still exposed for manual debugging if needed.
    ports:
      - "8025:8025"
    restart: unless-stopped

  grafana:
    image: grafana/grafana:11.1.0
    volumes:
      - ./deploy/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ./deploy/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./deploy/grafana/dashboards:/etc/grafana/dashboards:ro
      - ./deploy/grafana/provisioning/alerting:/etc/grafana/provisioning/alerting:ro
      - ./deploy/grafana/grafana.ini:/etc/grafana/grafana.ini:ro
    environment:
      - GF_SERVER_ROOT_URL=http://localhost:8088/grafana/
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      - GF_AUTH_DISABLE_LOGIN_FORM=true
      - GF_AUTH_PROXY_ENABLED=true
      - GF_AUTH_PROXY_HEADER_NAME=X-Forwarded-User
      - GF_AUTH_PROXY_AUTO_SIGN_UP=true
      - GF_USERS_AUTO_ASSIGN_ORG=true
      - GF_USERS_AUTO_ASSIGN_ORG_ROLE=Admin
      - GF_SMTP_ENABLED=true
      # Default to Mailpit for local & CI/e2e. To toggle to Gmail SMTP in a
      # different environment, override GF_SMTP_HOST / GF_SMTP_USER /
      # GF_SMTP_PASSWORD via environment or a compose override file.
      - GF_SMTP_HOST=mailpit:1025
      - GF_SMTP_FROM_ADDRESS=alerts@ai-cv-evaluator.local
      - GF_SMTP_FROM_NAME=AI CV Evaluator Alerts
      - GF_SMTP_SKIP_VERIFY=true
      - GF_SMTP_STARTTLS_POLICY=NoStartTLS
    depends_on:
      - prometheus
      - loki

  # Keycloak (dev)
  keycloak:
    image: quay.io/keycloak/keycloak:25.0
    command: ["start-dev", "--import-realm"]
    environment:
      KEYCLOAK_ADMIN: ${ADMIN_USERNAME}
      KEYCLOAK_ADMIN_PASSWORD: ${ADMIN_PASSWORD}
    volumes:
      - ./deploy/keycloak/realm-aicv.dev.json:/opt/keycloak/data/import/realm-aicv.json:ro
    ports:
      - "8089:8080" # Keycloak admin on different port from portal
    restart: unless-stopped

  # oauth2-proxy front-auth for Nginx
  oauth2-proxy:
    image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
    environment:
      OAUTH2_PROXY_PROVIDER: oidc
      # Use public issuer on localhost:8089 for tokens & discovery metadata
      OAUTH2_PROXY_OIDC_ISSUER_URL: http://localhost:8089/realms/aicv
      OAUTH2_PROXY_LOGIN_URL: http://localhost:8089/realms/aicv/protocol/openid-connect/auth
      # Use internal keycloak hostname for backchannel token exchange only
      OAUTH2_PROXY_REDEEM_URL: http://keycloak:8080/realms/aicv/protocol/openid-connect/token
      OAUTH2_PROXY_CLIENT_ID: oauth2-proxy
      OAUTH2_PROXY_CLIENT_SECRET: ${OAUTH2_PROXY_CLIENT_SECRET:-oauth2-proxy-secret}
      OAUTH2_PROXY_COOKIE_SECRET: ${OAUTH2_PROXY_COOKIE_SECRET:-MTIzNDU2Nzg5MDEyMzQ1Njc4OTAxMjM0NTY3ODkwMTI=}
      OAUTH2_PROXY_COOKIE_SECURE: "false"
      OAUTH2_PROXY_COOKIE_SAMESITE: lax
      OAUTH2_PROXY_EMAIL_DOMAINS: "*"
      OAUTH2_PROXY_SET_XAUTHREQUEST: "true"
      OAUTH2_PROXY_WHITELIST_DOMAINS: .localhost,localhost:8088
      OAUTH2_PROXY_REDIRECT_URL: http://localhost:8088/oauth2/callback
      OAUTH2_PROXY_UPSTREAMS: static://200
      OAUTH2_PROXY_HTTP_ADDRESS: 0.0.0.0:4180
      # Skip OIDC discovery (container cannot reach localhost:8089); provide
      # JWKS URL explicitly for signature verification.
      OAUTH2_PROXY_INSECURE_OIDC_SKIP_ISSUER_VERIFICATION: "true"
      OAUTH2_PROXY_SKIP_OIDC_DISCOVERY: "true"
      OAUTH2_PROXY_OIDC_GROUPS_CLAIM: "email"
      OAUTH2_PROXY_SKIP_CLAIMS_FROM_PROFILE_URL: "true"
      OAUTH2_PROXY_OIDC_JWKS_URL: http://keycloak:8080/realms/aicv/protocol/openid-connect/certs
    depends_on:
      - keycloak
    restart: unless-stopped

  # Local nginx to serve portal and proxy to services (dev only)
  dev-nginx:
    image: nginx:alpine
    ports:
      - "8088:80"
    volumes:
      - ./deploy/nginx/dev-conf.d:/etc/nginx/conf.d:ro
      - ./deploy/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./deploy/portal:/usr/share/nginx/html:ro
    depends_on:
      - app
      - frontend
      - oauth2-proxy
      - keycloak
      - grafana
      - prometheus
      - jaeger
      - redpanda-console
    restart: unless-stopped

volumes:
  db_data:
  redis_data:
  redpanda_data:
  qdrant_data: {}
